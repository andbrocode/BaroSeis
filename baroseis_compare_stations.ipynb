{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b26487a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T00:35:24.926376Z",
     "start_time": "2022-09-13T00:35:24.923877Z"
    }
   },
   "source": [
    "# ROMY - Barometric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4ef94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-05T16:30:44.374156Z",
     "start_time": "2023-04-05T16:30:42.823021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import obspy as obs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.baroseis import baroseis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e92999",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70bf1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# date = \"20240315\"\n",
    "date = \"20240324\"\n",
    "# date = \"20240423\"\n",
    "# date = \"20240312\"\n",
    "\n",
    "component = \"E\"\n",
    "\n",
    "\n",
    "# select channel A = tilt, J = rotation rate, H = acceleration\n",
    "cha =\"*A*\"\n",
    "\n",
    "# load config\n",
    "config_fur = baroseis.load_from_yaml(f\"./data/config_FFBI_FUR_{date}_file.yaml\")\n",
    "config_romy = baroseis.load_from_yaml(f\"./data/config_FFBI_ROMY_{date}_file.yaml\")\n",
    "config_dromy = baroseis.load_from_yaml(f\"./data/config_FFBI_DROMY_{date}_file.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60995a8b",
   "metadata": {},
   "source": [
    "### Load Spatial Pressure Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3566958",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = obs.read(f\"./data/pressure_gradient_{date}.mseed\")\n",
    "\n",
    "gradient = gradient.trim(config_fur['tbeg'], config_fur['tend'])\n",
    "gradient = gradient.detrend(\"demean\")\n",
    "gradient = gradient.taper(0.05, \"cosine\")\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baroseis object\n",
    "bs_fur = baroseis(conf=config_fur)\n",
    "bs_romy = baroseis(conf=config_romy)\n",
    "bs_dromy = baroseis(conf=config_dromy)\n",
    "\n",
    "# Load data specified in config\n",
    "bs_fur.load_data()\n",
    "bs_romy.load_data()\n",
    "bs_dromy.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c267ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st = obs.Stream()\n",
    "\n",
    "st.append(bs_fur.st.select(channel=\"*DO\")[0])\n",
    "st.append(bs_fur.st.select(channel=\"*DH\")[0])\n",
    "\n",
    "st.append(bs_fur.st.select(component=component)[0])\n",
    "st.append(bs_romy.st.select(component=component)[0])\n",
    "st.append(bs_dromy.st.select(component=component)[0])\n",
    "\n",
    "# st.plot(equal_scale=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db48968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# band pass filer\n",
    "st.detrend(\"demean\")\n",
    "st.detrend(\"linear\")\n",
    "st.taper(0.05, \"cosine\")\n",
    "st.filter('bandpass', freqmin=0.0005, freqmax=0.03, zerophase=True, corners=4)\n",
    "\n",
    "# detrend\n",
    "st.detrend(\"demean\")\n",
    "\n",
    "# taper edges\n",
    "st.taper(0.1, \"cosine\")\n",
    "\n",
    "for tr in st:\n",
    "    if tr.stats.station == \"ROMY\":\n",
    "        # integrate rotation to tilt\n",
    "        tr.integrate(method=\"cumtrapz\") # method = \"cumtrapz\" or \"spline\"\n",
    "        tr.stats.channel = tr.stats.channel[0] + \"A\" + tr.stats.channel[-1]\n",
    "\n",
    "    elif tr.stats.station == \"FUR\" or tr.stats.station == \"DROMY\":\n",
    "        # convert acceleration to tilt\n",
    "        tr.stats.channel = tr.stats.channel[0] + \"A\" + tr.stats.channel[-1]\n",
    "        if tr.stats.channel[-1] in [\"N\", \"E\"]:\n",
    "            tr.data = -tr.data/9.81\n",
    "\n",
    "# trim waveforms\n",
    "st = st.trim(config_fur['tbeg'], config_fur['tend'])\n",
    "\n",
    "# detrend waveforms\n",
    "st.detrend(\"demean\")\n",
    "\n",
    "# taper edges\n",
    "st.taper(0.05, \"cosine\")\n",
    "\n",
    "# show new waveforms\n",
    "st.plot(equal_scale=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tilt(seis_stream, pressure_data):\n",
    "    \"\"\"\n",
    "    Simple model for predicting tilt/rotation from pressure data.\n",
    "    \n",
    "    Args:\n",
    "        seis_stream: Stream with seismic data\n",
    "        pressure_data: List of pressure arrays [P, H, DP, DH]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predicted_data, coefficients, variance_reduction, residuals\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get seismic data\n",
    "    seis_data = {}\n",
    "    stations = []\n",
    "    for tr in seis_stream:\n",
    "        stations.append(tr.stats.station)\n",
    "\n",
    "    for sta in stations:\n",
    "        try:\n",
    "            tr = seis_stream.select(station=sta).copy()[0]\n",
    "            seis_data[sta] = tr.data\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not seis_data:\n",
    "        raise ValueError(\"No seismic data found\")\n",
    "    \n",
    "    # Ensure all data has same length\n",
    "    data_length = len(pressure_data[0])\n",
    "    for comp in seis_data:\n",
    "        if len(seis_data[comp]) != data_length:\n",
    "            # Simple interpolation\n",
    "            from scipy.interpolate import interp1d\n",
    "            x_old = np.linspace(0, 1, len(seis_data[comp]))\n",
    "            x_new = np.linspace(0, 1, data_length)\n",
    "            f = interp1d(x_old, seis_data[comp], kind='linear', fill_value='extrapolate')\n",
    "            seis_data[comp] = f(x_new)\n",
    "    \n",
    "    # Create design matrix\n",
    "    A = np.column_stack(pressure_data)\n",
    "    \n",
    "    # Results\n",
    "    results = {\n",
    "        'original_data': {},\n",
    "        'predicted_data': {},\n",
    "        'coefficients': {},\n",
    "        'variance_reduction': {},\n",
    "        'residuals': {}\n",
    "    }\n",
    "    \n",
    "    # Process each component\n",
    "    for comp, seis_comp_data in seis_data.items():\n",
    "        # Least squares: A * x = b\n",
    "        coefficients = np.linalg.lstsq(A, seis_comp_data, rcond=None)[0]\n",
    "        predicted_data = A @ coefficients\n",
    "        \n",
    "        # Variance reduction\n",
    "        original_var = np.var(seis_comp_data)\n",
    "        residual_var = np.var(seis_comp_data - predicted_data)\n",
    "        var_reduction = ((original_var - residual_var) / original_var) * 100\n",
    "        \n",
    "        # Store\n",
    "        results['original_data'][comp] = seis_comp_data\n",
    "        results['predicted_data'][comp] = predicted_data\n",
    "        results['coefficients'][comp] = coefficients\n",
    "        results['variance_reduction'][comp] = var_reduction\n",
    "        results['residuals'][comp] = seis_comp_data - predicted_data\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd668085",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8375a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seis_stream = st.select(channel=cha).copy()\n",
    "model_data = [\n",
    "    st.select(channel=\"*DO\").copy()[0].data,\n",
    "    st.select(channel=\"*DH\").copy()[0].data,\n",
    "]\n",
    "\n",
    "# Run model\n",
    "model1 = model_tilt(seis_stream, model_data)\n",
    "\n",
    "# Access results\n",
    "print(\"Variance reduction:\")\n",
    "for comp in model1['predicted_data']:\n",
    "    print(f\"{comp}: {model1['variance_reduction'][comp]:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e0d7b",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "seis_stream = st.select(channel=cha).copy()\n",
    "pressure_data = [\n",
    "    st.select(channel=\"*DO\").copy()[0].data,\n",
    "    st.select(channel=\"*DH\").copy()[0].data,\n",
    "    st.select(channel=\"*DO\").copy().differentiate()[0].data,\n",
    "    st.select(channel=\"*DH\").copy().differentiate()[0].data\n",
    "]\n",
    "\n",
    "# Run model\n",
    "model2 = model_tilt(seis_stream, pressure_data)\n",
    "\n",
    "# Access results\n",
    "print(\"Variance reduction:\")\n",
    "for comp in model2['predicted_data']:\n",
    "    print(f\"{comp}: {model2['variance_reduction'][comp]:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49383e4c",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_multiple_stations(seis_streams, pressure_streams, coeff_files, station_names, component):\n",
    "    \"\"\"\n",
    "    Model multiple stations using their respective coefficient files.\n",
    "    Output structure: {'original_data': {'station': array}, 'predicted_data': {'station': array}, ...}\n",
    "    \"\"\"\n",
    "    import yaml\n",
    "    import numpy as np\n",
    "    \n",
    "    comp = component\n",
    "\n",
    "    # Initialize results with stations as second-level keys\n",
    "    results = {\n",
    "        'original_data': {},\n",
    "        'predicted_data': {},\n",
    "        'coefficients': {},\n",
    "        'variance_reduction': {},\n",
    "        'residuals': {}\n",
    "    }\n",
    "    \n",
    "    for seis_stream, pressure_stream, coeff_file, station_name in zip(\n",
    "        seis_streams, pressure_streams, coeff_files, station_names):\n",
    "        \n",
    "        # Load coefficients\n",
    "        with open(coeff_file, 'r') as f:\n",
    "            coeffs = yaml.safe_load(f)\n",
    "        \n",
    "        # Get pressure data\n",
    "        pressure_data = {}\n",
    "        for tr in pressure_stream:\n",
    "            if tr.stats.channel.endswith('DH'):\n",
    "                pressure_data['H'] = tr.data\n",
    "            elif tr.stats.channel.endswith('DO'):\n",
    "                pressure_data['P'] = tr.data\n",
    "        \n",
    "        # Process each component and combine results\n",
    "        station_original = []\n",
    "        station_predicted = []\n",
    "        station_coeffs = []\n",
    "        station_residuals = []\n",
    "        \n",
    "        try:\n",
    "            # Get seismic data\n",
    "            tr = seis_stream.select(channel=f\"*{comp}\").copy()[0]\n",
    "            original_data = tr.data\n",
    "            \n",
    "            # Get coefficients\n",
    "            comp_coeffs = coeffs[comp]\n",
    "            coeff_H = comp_coeffs.get('coeff_H', 0) * 1e-11 # turn from nrad/hPa to rad/Pa\n",
    "            coeff_P = comp_coeffs.get('coeff_P', 0) * 1e-11 # turn from nrad/hPa to rad/Pa\n",
    "            \n",
    "            # Model prediction\n",
    "            predicted_data = (coeff_H * np.array(pressure_data['H']) + \n",
    "                            coeff_P * np.array(pressure_data['P']))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {station_name} {comp}: {e}\")\n",
    "            continue\n",
    "\n",
    "        results['original_data'][station_name] = original_data\n",
    "        results['predicted_data'][station_name] = predicted_data\n",
    "        results['coefficients'][station_name] = [coeff_P, coeff_H]\n",
    "        results['residuals'][station_name] = original_data - predicted_data\n",
    "        \n",
    "        # Calculate variance reduction for the combined station data\n",
    "        var_orig = np.var(results['original_data'][station_name])\n",
    "        var_residual = np.var(results['residuals'][station_name])\n",
    "        var_reduction = ((var_orig - var_residual) / var_orig) * 100\n",
    "        results['variance_reduction'][station_name] = var_reduction\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fec59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_stream = obs.Stream()\n",
    "pressure_stream.append(st.select(channel=\"*DO\").copy()[0])\n",
    "pressure_stream.append(st.select(channel=\"*DH\").copy()[0])\n",
    "\n",
    "seis_stream_dromy = st.select(station=\"DROMY\").copy()\n",
    "seis_stream_romy = st.select(station=\"ROMY\").copy()\n",
    "seis_stream_fur = st.select(station=\"FUR\").copy()\n",
    "\n",
    "\n",
    "model3 = model_multiple_stations(\n",
    "    [seis_stream_dromy, seis_stream_romy, seis_stream_fur],\n",
    "    [pressure_stream, pressure_stream, pressure_stream], \n",
    "    ['data/statsmodel_DROMY.yaml', 'data/statsmodel_ROMY.yaml', 'data/statsmodel_FUR.yaml'],\n",
    "    ['DROMY', 'ROMY', 'FUR'],\n",
    "    component\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad49454",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Adaptive Model': model1,\n",
    "    'Statistic Model': model3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform_comparison(model_results,\n",
    "                           time_unit='minutes', residual=False, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Simple plot with vertical subplots showing waveforms for each station.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary with model names as keys and results as values.\n",
    "                      Each result should contain 'original_data' and 'predicted_data' for each station.\n",
    "        time_unit: Time unit for x-axis\n",
    "        residual: Boolean to plot residuals\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get time scaling\n",
    "    tscale_dict = {\"hours\": 1/3600, \"days\": 1/86400, \"minutes\": 1/60, \"seconds\": 1}\n",
    "    tscale = tscale_dict.get(time_unit, 1/60)\n",
    "    \n",
    "    # Get the first model to determine units and scaling\n",
    "    first_model = list(model_results.values())[0]\n",
    "    first_station = list(first_model['original_data'].keys())[0]\n",
    "    \n",
    "    # Set units and scaling based on the data\n",
    "    # You might need to adjust this based on your data structure\n",
    "    ylabel = \"Amplitude\"\n",
    "    yscale = 1e9  # Adjust based on your data units\n",
    "    \n",
    "    # Get unique stations from the first model's data\n",
    "    stations = list(first_model['original_data'].keys())\n",
    "    stations.sort()  # Sort for consistent ordering\n",
    "    \n",
    "    font = 14\n",
    "\n",
    "    # Create subplots for each station\n",
    "    fig, axes = plt.subplots(len(stations), 1, figsize=figsize, sharex=True)\n",
    "    \n",
    "    # Handle case where there's only one station\n",
    "    if len(stations) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Colors for different models\n",
    "    colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray']\n",
    "    \n",
    "    # Plot each station\n",
    "    for i, station in enumerate(stations):\n",
    "        try:\n",
    "            # Get original data for this station from the first model\n",
    "            orig_data = first_model['original_data'][station] * yscale\n",
    "            \n",
    "            # Create time array (assuming uniform sampling)\n",
    "            # You might need to adjust this based on your data structure\n",
    "            n_samples = len(orig_data)\n",
    "            dt = 1.0  # Adjust this based on your sampling rate\n",
    "            times = np.arange(n_samples) * dt * tscale\n",
    "            \n",
    "            # Plot original data\n",
    "            axes[i].plot(times, orig_data, 'k-', linewidth=2, \n",
    "                        label=f'{station}-{component}', alpha=1, zorder=1)\n",
    "            max_orig = np.max(np.abs(orig_data))\n",
    "\n",
    "            # Plot each model for this station\n",
    "            for j, (model_name, results) in enumerate(model_results.items()):\n",
    "                if station in results.get('predicted_data', {}):\n",
    "                    pred_data = results['predicted_data'][station] * yscale\n",
    "                    var_reduction = results.get('variance_reduction', {}).get(station, 0)\n",
    "                    \n",
    "                    color = colors[j % len(colors)]\n",
    "                    if residual:\n",
    "                        res_data = orig_data - pred_data\n",
    "                        axes[i].plot(times, res_data, color=color, linewidth=1.5, zorder=2,\n",
    "                                label=f'{model_name} (VR: {var_reduction:.1f}%)', alpha=0.9)\n",
    "                        # find y absolute maximum for ylim\n",
    "                        y_max = np.max([np.max(np.abs(res_data)), max_orig])*1.01\n",
    "                        axes[i].set_ylim(-y_max, y_max)\n",
    "                    else:\n",
    "                        axes[i].plot(times, pred_data, color=color, linewidth=1.5, zorder=2,\n",
    "                                label=f'{model_name} (VR: {var_reduction:.1f}%)', alpha=0.9)\n",
    "                        # find y absolute maximum for ylim\n",
    "                        y_max = np.max([np.max(np.abs(pred_data)), max_orig])*1.01\n",
    "                        axes[i].set_ylim(-y_max, y_max)\n",
    "            \n",
    "            # Format subplot\n",
    "            axes[i].set_ylabel(f\"{ylabel}\", fontsize=font)\n",
    "            axes[i].set_title(f\"Station {station}\", fontsize=font, fontweight='bold')\n",
    "            axes[i].legend(fontsize=font-2, ncol=2)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].tick_params(labelsize=font-1)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting station {station}: {e}\")\n",
    "            axes[i].text(0.5, 0.5, f'Error loading station {station}', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            continue\n",
    "    \n",
    "    # Set x-axis label\n",
    "    axes[-1].set_xlabel(f\"Time ({time_unit})\", fontsize=font)\n",
    "    \n",
    "    # Set overall title\n",
    "    # if residual:\n",
    "    #     title = f\"Residual Comparison - Stations\"\n",
    "    # else:\n",
    "    #     title = f\"Model Comparison - Stations\"\n",
    "    # fig.suptitle(title, fontsize=font+2, fontweight='bold')\n",
    "    \n",
    "    # # Add model names as text outside the frame\n",
    "    # model_names = list(model_results.keys())\n",
    "    # model_text = \"Models:  \" + \",   \".join([f\"{name}\" for i, name in enumerate(model_names)])\n",
    "    \n",
    "    # # Add text below the plot\n",
    "    # fig.text(0.5, 0.02, model_text, ha='center', va='bottom', fontsize=font-2, \n",
    "    #          bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Adjust layout to make room for the model text\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ab37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_waveform_comparison(models, residual=False)\n",
    "\n",
    "fig.savefig(f\"./figures/station_comparison/{date}_{component}_station_waveform_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "fig = plot_waveform_comparison(models, residual=True)\n",
    "\n",
    "fig.savefig(f\"./figures/station_comparison/{date}_{component}_station_waveform_comparison_residual.png\", dpi=150, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d552a1c",
   "metadata": {},
   "source": [
    "### Create Output Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b987f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_table(model_results, date, station_names, components, filename):\n",
    "    \"\"\"\n",
    "    Save model results to CSV file, updating existing data if file exists.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary with model results\n",
    "        date: Date string\n",
    "        station_names: List of station names\n",
    "        components: List of components\n",
    "        filename: CSV file path to save to\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Read existing data if file exists\n",
    "    if os.path.exists(filename):\n",
    "        existing_df = pd.read_csv(filename)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "    \n",
    "    # Create new data\n",
    "    rows = []\n",
    "    models = model_results if isinstance(model_results, dict) and 'variance_reduction' not in model_results else {'model1': model_results}\n",
    "    \n",
    "    for model_name, results in models.items():\n",
    "        for station in station_names:\n",
    "            for comp in components:\n",
    "                vr = results.get('variance_reduction', {}).get(station, 0)\n",
    "                rows.append({\n",
    "                    'Date': date,\n",
    "                    'Station': station,\n",
    "                    'Component': comp,\n",
    "                    'Model': model_name,\n",
    "                    'Variance_Reduction': vr\n",
    "                })\n",
    "    \n",
    "    new_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Remove old entries for same date/station/component/model\n",
    "    if not existing_df.empty:\n",
    "        mask = ~((existing_df['Date'] == date) & \n",
    "                (existing_df['Station'].isin(station_names)) & \n",
    "                (existing_df['Component'].isin(components)) & \n",
    "                (existing_df['Model'].isin(models.keys())))\n",
    "        existing_df = existing_df[mask]\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "    \n",
    "    # remove identical rows\n",
    "    combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "    # sorting\n",
    "    combined_df = combined_df.sort_values(by=['Date', 'Station', 'Component', 'Model'])\n",
    "\n",
    "    # Save to file\n",
    "    combined_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(combined_df)} rows to {filename}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cab68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 rows to ./data/model_stations_vr_results.csv\n",
      "        Date Station Component            Model  Variance_Reduction\n",
      "0   20240315   DROMY         E   Adaptive Model           64.501771\n",
      "1   20240315   DROMY         E  Statistic Model           63.990977\n",
      "2   20240315   DROMY         N   Adaptive Model           12.147146\n",
      "3   20240315   DROMY         N  Statistic Model           -0.099776\n",
      "4   20240315     FUR         E   Adaptive Model           30.827652\n",
      "5   20240315     FUR         E  Statistic Model         -120.975325\n",
      "6   20240315     FUR         N   Adaptive Model           45.919098\n",
      "7   20240315     FUR         N  Statistic Model           45.460772\n",
      "8   20240315    ROMY         E   Adaptive Model           96.908900\n",
      "9   20240315    ROMY         E  Statistic Model           54.840690\n",
      "10  20240315    ROMY         N   Adaptive Model           96.644728\n",
      "11  20240315    ROMY         N  Statistic Model          -66.624111\n",
      "12  20240324   DROMY         E   Adaptive Model           75.388547\n",
      "14  20240324   DROMY         E  Statistic Model           74.299041\n",
      "16  20240324   DROMY         N   Adaptive Model           22.042869\n",
      "17  20240324   DROMY         N  Statistic Model           10.439171\n",
      "18  20240324     FUR         E   Adaptive Model           53.822144\n",
      "20  20240324     FUR         E  Statistic Model         -150.448106\n",
      "22  20240324     FUR         N   Adaptive Model           56.896871\n",
      "23  20240324     FUR         N  Statistic Model           56.693854\n",
      "24  20240324    ROMY         E   Adaptive Model           79.201455\n",
      "26  20240324    ROMY         E  Statistic Model           49.464556\n",
      "28  20240324    ROMY         N   Adaptive Model           88.007476\n",
      "29  20240324    ROMY         N  Statistic Model          -85.686637\n",
      "38  20240324   DROMY         E   Adaptive Model           75.388547\n",
      "41  20240324   DROMY         E  Statistic Model           74.299041\n",
      "37  20240324     FUR         E   Adaptive Model           53.822144\n",
      "40  20240324     FUR         E  Statistic Model         -150.448106\n",
      "36  20240324    ROMY         E   Adaptive Model           79.201455\n",
      "39  20240324    ROMY         E  Statistic Model           49.464556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "station_names = ['ROMY', 'FUR', 'DROMY']\n",
    "\n",
    "# Create table\n",
    "results_table = results_table(models, date, station_names, [component], \"./data/model_stations_vr_results.csv\")\n",
    "print(results_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f051090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latex_table(df, filename, caption=\"Model Results\"):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to LaTeX table and save to file.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        filename: Output .tex file path\n",
    "        caption: Table caption\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Format variance reduction to 1 decimal place\n",
    "    df_formatted = df.copy()\n",
    "    if 'Variance_Reduction' in df_formatted.columns:\n",
    "        df_formatted['Variance_Reduction'] = df_formatted['Variance_Reduction'].round(1)\n",
    "    \n",
    "    # Generate LaTeX table\n",
    "    latex_code = df_formatted.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        caption=caption,\n",
    "        label=\"tab:results\",\n",
    "        position='htbp'\n",
    "    )\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(latex_code)\n",
    "    \n",
    "    print(f\"LaTeX table saved to {filename}\")\n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "806f2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latex_table(df, filename, caption=\"Model Results\"):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to LaTeX table with multirow support for combined cells.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Format and sort DataFrame\n",
    "    df_formatted = df.copy()\n",
    "    if 'Variance_Reduction' in df_formatted.columns:\n",
    "        df_formatted['Variance_Reduction'] = df_formatted['Variance_Reduction'].round(1)\n",
    "    \n",
    "    df_sorted = df_formatted.sort_values(['Date', 'Station', 'Component'])\n",
    "    \n",
    "    # Count occurrences for multirow\n",
    "    date_counts = df_sorted.groupby('Date').size()\n",
    "    station_counts = df_sorted.groupby(['Date', 'Station']).size()\n",
    "    component_counts = df_sorted.groupby(['Date', 'Station', 'Component']).size()\n",
    "    \n",
    "    latex_lines = []\n",
    "    latex_lines.append(\"\\\\begin{table}[htbp]\")\n",
    "    latex_lines.append(\"\\\\centering\")\n",
    "    latex_lines.append(f\"\\\\caption{{{caption}}}\")\n",
    "    latex_lines.append(\"\\\\label{tab:results}\")\n",
    "    latex_lines.append(\"\\\\begin{tabular}{lcccc}\")\n",
    "    latex_lines.append(\"\\\\toprule\")\n",
    "    latex_lines.append(\"Date & Station & Component & Model & Variance Reduction \\\\\\\\\")\n",
    "    latex_lines.append(\"\\\\midrule\")\n",
    "    \n",
    "    # Track previous values\n",
    "    prev_date = None\n",
    "    prev_station = None\n",
    "    prev_component = None\n",
    "    \n",
    "    for _, row in df_sorted.iterrows():\n",
    "        current_date = row['Date']\n",
    "        current_station = row['Station']\n",
    "        current_component = row['Component']\n",
    "        model = row['Model']\n",
    "        vr = row['Variance_Reduction']\n",
    "        \n",
    "        # Determine what to show\n",
    "        if current_date != prev_date:\n",
    "            date_col = f\"\\\\multirow{{{date_counts[current_date]}}}{{*}}{{{current_date}}}\"\n",
    "        else:\n",
    "            date_col = \"\"\n",
    "            \n",
    "        if current_station != prev_station or current_date != prev_date:\n",
    "            station_col = f\"\\\\multirow{{{station_counts[(current_date, current_station)]}}}{{*}}{{{current_station}}}\"\n",
    "        else:\n",
    "            station_col = \"\"\n",
    "            \n",
    "        if current_component != prev_component or current_station != prev_station or current_date != prev_date:\n",
    "            component_col = f\"\\\\multirow{{{component_counts[(current_date, current_station, current_component)]}}}{{*}}{{{current_component}}}\"\n",
    "        else:\n",
    "            component_col = \"\"\n",
    "        \n",
    "        # Add row\n",
    "        latex_lines.append(f\"{date_col} & {station_col} & {component_col} & {model} & {vr} \\\\\\\\\")\n",
    "        \n",
    "        # Update previous values\n",
    "        prev_date = current_date\n",
    "        prev_station = current_station\n",
    "        prev_component = current_component\n",
    "    \n",
    "    latex_lines.append(\"\\\\bottomrule\")\n",
    "    latex_lines.append(\"\\\\end{tabular}\")\n",
    "    latex_lines.append(\"\\\\end{table}\")\n",
    "    \n",
    "    latex_code = \"\\n\".join(latex_lines)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(latex_code)\n",
    "    \n",
    "    print(f\"LaTeX table with multirow saved to {filename}\")\n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table with multirow saved to ./data/results.tex\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\caption{Model Variance Reduction Results}\\n\\\\label{tab:results}\\n\\\\begin{tabular}{lcccc}\\n\\\\toprule\\nDate & Station & Component & Model & Variance Reduction \\\\\\\\\\n\\\\midrule\\n\\\\multirow{12}{*}{20240315} & \\\\multirow{4}{*}{DROMY} & \\\\multirow{2}{*}{E} & Adaptive Model & 64.5 \\\\\\\\\\n &  &  & Statistic Model & 64.0 \\\\\\\\\\n &  & \\\\multirow{2}{*}{N} & Adaptive Model & 12.1 \\\\\\\\\\n &  &  & Statistic Model & -0.1 \\\\\\\\\\n & \\\\multirow{4}{*}{FUR} & \\\\multirow{2}{*}{E} & Adaptive Model & 30.8 \\\\\\\\\\n &  &  & Statistic Model & -121.0 \\\\\\\\\\n &  & \\\\multirow{2}{*}{N} & Adaptive Model & 45.9 \\\\\\\\\\n &  &  & Statistic Model & 45.5 \\\\\\\\\\n & \\\\multirow{4}{*}{ROMY} & \\\\multirow{2}{*}{E} & Adaptive Model & 96.9 \\\\\\\\\\n &  &  & Statistic Model & 54.8 \\\\\\\\\\n &  & \\\\multirow{2}{*}{N} & Adaptive Model & 96.6 \\\\\\\\\\n &  &  & Statistic Model & -66.6 \\\\\\\\\\n\\\\multirow{12}{*}{20240324} & \\\\multirow{4}{*}{DROMY} & \\\\multirow{2}{*}{E} & Adaptive Model & 75.4 \\\\\\\\\\n &  &  & Statistic Model & 74.3 \\\\\\\\\\n &  & \\\\multirow{2}{*}{N} & Adaptive Model & 22.0 \\\\\\\\\\n &  &  & Statistic Model & 10.4 \\\\\\\\\\n & \\\\multirow{4}{*}{FUR} & \\\\multirow{2}{*}{E} & Adaptive Model & 53.8 \\\\\\\\\\n &  &  & Statistic Model & -150.4 \\\\\\\\\\n &  & \\\\multirow{2}{*}{N} & Adaptive Model & 56.9 \\\\\\\\\\n &  &  & Statistic Model & 56.7 \\\\\\\\\\n & \\\\multirow{4}{*}{ROMY} & \\\\multirow{2}{*}{E} & Adaptive Model & 79.2 \\\\\\\\\\n &  &  & Statistic Model & 49.5 \\\\\\\\\\n &  & \\\\multirow{2}{*}{N} & Adaptive Model & 88.0 \\\\\\\\\\n &  &  & Statistic Model & -85.7 \\\\\\\\\\n\\\\multirow{6}{*}{20240324} & \\\\multirow{2}{*}{DROMY} & \\\\multirow{2}{*}{E} & Adaptive Model & 75.4 \\\\\\\\\\n &  &  & Statistic Model & 74.3 \\\\\\\\\\n & \\\\multirow{2}{*}{FUR} & \\\\multirow{2}{*}{E} & Adaptive Model & 53.8 \\\\\\\\\\n &  &  & Statistic Model & -150.4 \\\\\\\\\\n & \\\\multirow{2}{*}{ROMY} & \\\\multirow{2}{*}{E} & Adaptive Model & 79.2 \\\\\\\\\\n &  &  & Statistic Model & 49.5 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_latex_table(results_table, './data/results.tex', \"Model Variance Reduction Results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9800cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectra_comparison(seis_stream, model_results,\n",
    "                           method='welch', fmin=0.0005, fmax=0.03,\n",
    "                           log_scale=True, db_scale=False, residual=False, \n",
    "                           smooth_octave=False, octave_fraction=1/3, \n",
    "                           smooth_method='median', figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot spectra comparison for 3 components (Z, N, E) showing spectra for each model.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from scipy.signal import welch\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    from scipy.signal import windows\n",
    "\n",
    "    def smooth_octave_bands(freq, spectrum, octave_fraction=1/3, method='median'):\n",
    "        \"\"\"Smooth spectrum in fractional octave bands.\"\"\"\n",
    "        # Create octave band centers\n",
    "        f_min = freq[freq > 0].min()\n",
    "        f_max = freq.max()\n",
    "        \n",
    "        # Calculate number of octave bands\n",
    "        n_octaves = int(np.log2(f_max / f_min) / octave_fraction) + 1\n",
    "        \n",
    "        # Create octave band centers\n",
    "        f_centers = f_min * (2 ** (octave_fraction * np.arange(n_octaves)))\n",
    "        \n",
    "        # Calculate band edges\n",
    "        f_lower = f_centers / (2 ** (octave_fraction / 2))\n",
    "        f_upper = f_centers * (2 ** (octave_fraction / 2))\n",
    "        \n",
    "        # Initialize smoothed arrays\n",
    "        freq_smooth = []\n",
    "        spectrum_smooth = []\n",
    "        \n",
    "        for i in range(len(f_centers)):\n",
    "            # Find frequencies within this octave band\n",
    "            mask = (freq >= f_lower[i]) & (freq <= f_upper[i])\n",
    "            \n",
    "            if np.any(mask):\n",
    "                if method == 'median':\n",
    "                    smooth_val = np.median(spectrum[mask])\n",
    "                else:  # mean\n",
    "                    smooth_val = np.mean(spectrum[mask])\n",
    "                \n",
    "                freq_smooth.append(f_centers[i])\n",
    "                spectrum_smooth.append(smooth_val)\n",
    "        \n",
    "        return np.array(freq_smooth), np.array(spectrum_smooth)\n",
    "\n",
    "    # Set units and scaling\n",
    "    channel_type = seis_stream[0].stats.channel[1]\n",
    "    if channel_type == 'J':\n",
    "        ylabel = \"Rotation Rate\\n(nrad/s)\"\n",
    "        yscale = 1e9\n",
    "    elif channel_type == 'A':\n",
    "        ylabel = \"Tilt (nrad)\"\n",
    "        yscale = 1e9\n",
    "    elif channel_type == 'H':\n",
    "        ylabel = \"Acceleration\\n(nm/s²)\"\n",
    "        yscale = 1e9\n",
    "    else:\n",
    "        ylabel = \"Amplitude\"\n",
    "        yscale = 1.0\n",
    "\n",
    "    components = ['Z', 'N', 'E']\n",
    "    font = 14\n",
    "\n",
    "    # Create 3 vertical subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize, sharex=True)\n",
    "    \n",
    "    # Colors for different models\n",
    "    colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray']\n",
    "    \n",
    "    # Plot each component\n",
    "    for i, comp in enumerate(components):\n",
    "        try:\n",
    "            # Get original data\n",
    "            orig_tr = seis_stream.select(channel=f\"*{comp}\").copy()[0]\n",
    "            \n",
    "            # Compute original spectrum\n",
    "            if method.lower() == 'fft':\n",
    "                n = len(orig_tr.data)\n",
    "                win = windows.hann(n)\n",
    "                spec_orig = fft(orig_tr.data * win)\n",
    "                freq = fftfreq(n, d=orig_tr.stats.delta)\n",
    "                pos_freq = freq[0:n//2]\n",
    "                mag_orig = np.abs(spec_orig[0:n//2]) * 2.0/n * yscale\n",
    "            else:  # welch\n",
    "                nperseg = int(orig_tr.stats.sampling_rate * 3600)  # 1-hour segments\n",
    "                noverlap = nperseg // 2\n",
    "                freq, psd_orig = welch(orig_tr.data, fs=orig_tr.stats.sampling_rate,\n",
    "                                     window='hann', nperseg=nperseg, noverlap=noverlap)\n",
    "                mag_orig = np.sqrt(psd_orig) * yscale\n",
    "                pos_freq = freq\n",
    "            \n",
    "            # Apply octave band smoothing if requested\n",
    "            if smooth_octave:\n",
    "                pos_freq, mag_orig = smooth_octave_bands(pos_freq, mag_orig, \n",
    "                                                        octave_fraction, smooth_method)\n",
    "            \n",
    "            # Convert to dB if requested\n",
    "            if db_scale:\n",
    "                mag_orig = 20 * np.log10(mag_orig)\n",
    "                ylabel_comp = ylabel + \" (dB)\"\n",
    "            else:\n",
    "                ylabel_comp = ylabel\n",
    "            \n",
    "            # Apply frequency limits\n",
    "            mask = (pos_freq >= fmin) & (pos_freq <= fmax)\n",
    "            pos_freq_plot = pos_freq[mask]\n",
    "            mag_orig_plot = mag_orig[mask]\n",
    "            \n",
    "            # Plot original spectrum\n",
    "            axes[i].plot(pos_freq_plot, mag_orig_plot, 'k-', linewidth=2, \n",
    "                        label=f'{comp}-Component', alpha=1, zorder=1)\n",
    "\n",
    "            # Plot each model\n",
    "            for j, (model_name, results) in enumerate(model_results.items()):\n",
    "                if comp in results['predicted_data']:\n",
    "                    residual_tr = seis_stream.select(channel=f\"*{comp}\").copy()[0]\n",
    "                    residual_tr.data = seis_stream.select(channel=f\"*{comp}\").copy()[0].data - results['predicted_data'][comp]\n",
    "                    var_reduction = results['variance_reduction'][comp]\n",
    "                    \n",
    "                    # Compute predicted spectrum\n",
    "                    if method.lower() == 'fft':\n",
    "                        n = len(residual_tr.data)\n",
    "                        win = windows.hann(n)\n",
    "                        spec_pred = fft(residual_tr.data * win)\n",
    "                        mag_pred = np.abs(spec_pred[0:n//2]) * 2.0/n * yscale\n",
    "                        freq_pred = fftfreq(n, d=residual_tr.stats.delta)[0:n//2]\n",
    "                    else:  # welch\n",
    "                        nperseg = int(residual_tr.stats.sampling_rate * 3600)\n",
    "                        noverlap = nperseg // 2\n",
    "                        freq_pred, psd_pred = welch(residual_tr.data, fs=residual_tr.stats.sampling_rate,\n",
    "                                                  window='hann', nperseg=nperseg, noverlap=noverlap)\n",
    "                        mag_pred = np.sqrt(psd_pred) * yscale\n",
    "                    \n",
    "                    # Apply octave band smoothing if requested\n",
    "                    if smooth_octave:\n",
    "                        freq_pred, mag_pred = smooth_octave_bands(freq_pred, mag_pred, \n",
    "                                                                octave_fraction, smooth_method)\n",
    "                    \n",
    "                    # Convert to dB if requested\n",
    "                    if db_scale:\n",
    "                        mag_pred = 20 * np.log10(mag_pred)\n",
    "                    \n",
    "                    # Apply frequency limits\n",
    "                    mask_pred = (freq_pred >= fmin) & (freq_pred <= fmax)\n",
    "                    pos_freq_pred_plot = freq_pred[mask_pred]\n",
    "                    mag_pred_plot = mag_pred[mask_pred]\n",
    "                    \n",
    "                    color = colors[j % len(colors)]\n",
    "                    if residual:\n",
    "                        # For residuals, we need to interpolate to match frequencies\n",
    "                        from scipy.interpolate import interp1d\n",
    "                        f_interp = interp1d(pos_freq_plot, mag_orig_plot, kind='linear', \n",
    "                                          bounds_error=False, fill_value=0)\n",
    "                        mag_orig_interp = f_interp(pos_freq_pred_plot)\n",
    "                        res_spectrum = mag_orig_interp - mag_pred_plot\n",
    "                        axes[i].plot(pos_freq_pred_plot, res_spectrum, color=color, linewidth=1.5, zorder=2,\n",
    "                                label=f'M{j+1} (VR: {var_reduction:.1f}%)', alpha=0.9)\n",
    "                    else:\n",
    "                        axes[i].plot(pos_freq_pred_plot, mag_pred_plot, color=color, linewidth=1.5, zorder=2,\n",
    "                                label=f'M{j+1} (VR: {var_reduction:.1f}%)', alpha=0.9)\n",
    "            \n",
    "            # Format subplot\n",
    "            axes[i].set_ylabel(r\"ASD nrad/$\\sqrt{Hz}$\", fontsize=font)\n",
    "            axes[i].legend(loc='lower left', fontsize=font-2, ncol=2)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].tick_params(labelsize=font-1)\n",
    "            \n",
    "            # Set scales\n",
    "            if log_scale:\n",
    "                axes[i].set_xscale('log')\n",
    "                if not db_scale:  # Only use log scale for y-axis if not in dB\n",
    "                    axes[i].set_yscale('log')\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting {comp}: {e}\")\n",
    "            axes[i].text(0.5, 0.5, f'Error loading {comp}', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            continue\n",
    "    \n",
    "    # Set x-axis label\n",
    "    axes[-1].set_xlabel(\"Frequency (Hz)\", fontsize=font)\n",
    "    \n",
    "    # Set overall title\n",
    "    if residual:\n",
    "        title = f\"Spectra Residual Comparison\"\n",
    "    else:\n",
    "        title = f\"Spectra Model Comparison\"\n",
    "    fig.suptitle(title, fontsize=font+2, fontweight='bold')\n",
    "    \n",
    "    # Add model names as text outside the frame\n",
    "    model_names = list(model_results.keys())\n",
    "    model_text = \"Models:  \" + \",   \".join([f\"{name}\" for i, name in enumerate(model_names)])\n",
    "    \n",
    "    # Add text below the plot\n",
    "    fig.text(0.5, 0.02, model_text, ha='center', va='bottom', fontsize=font-2, \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Adjust layout to make room for the model text\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_spectra_comparison(seis_stream, model_results, method='fft', db_scale=False)\n",
    "\n",
    "# fig = plot_spectra_comparison(seis_stream, model_results, method='fft',\n",
    "#                              smooth_octave=True, octave_fraction=1/6, smooth_method='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_spectra_comparison(seis_stream, model_results, fmin=0.0005, fmax=0.03, \n",
    "                           smooth_octave=False, octave_fraction=1/3, smooth_method='median', \n",
    "                           figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot spectral difference comparison for 3 components (Z, N, E) showing difference of spectra in dB.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    from scipy.signal import windows\n",
    "\n",
    "    def smooth_octave_bands(freq, spectrum, octave_fraction=1/3, method='median'):\n",
    "        \"\"\"Smooth spectrum in fractional octave bands.\"\"\"\n",
    "        # Create octave band centers\n",
    "        f_min = freq[freq > 0].min()\n",
    "        f_max = freq.max()\n",
    "        \n",
    "        # Calculate number of octave bands\n",
    "        n_octaves = int(np.log2(f_max / f_min) / octave_fraction) + 1\n",
    "        \n",
    "        # Create octave band centers\n",
    "        f_centers = f_min * (2 ** (octave_fraction * np.arange(n_octaves)))\n",
    "        \n",
    "        # Calculate band edges\n",
    "        f_lower = f_centers / (2 ** (octave_fraction / 2))\n",
    "        f_upper = f_centers * (2 ** (octave_fraction / 2))\n",
    "        \n",
    "        # Initialize smoothed arrays\n",
    "        freq_smooth = []\n",
    "        spectrum_smooth = []\n",
    "        \n",
    "        for i in range(len(f_centers)):\n",
    "            # Find frequencies within this octave band\n",
    "            mask = (freq >= f_lower[i]) & (freq <= f_upper[i])\n",
    "            \n",
    "            if np.any(mask):\n",
    "                if method == 'median':\n",
    "                    smooth_val = np.median(spectrum[mask])\n",
    "                else:  # mean\n",
    "                    smooth_val = np.mean(spectrum[mask])\n",
    "                \n",
    "                freq_smooth.append(f_centers[i])\n",
    "                spectrum_smooth.append(smooth_val)\n",
    "        \n",
    "        return np.array(freq_smooth), np.array(spectrum_smooth)\n",
    "\n",
    "    # Set units and scaling\n",
    "    channel_type = seis_stream[0].stats.channel[1]\n",
    "    if channel_type == 'J':\n",
    "        ylabel = \"Rotation Rate\\n(nrad/s)\"\n",
    "        yscale = 1e9\n",
    "    elif channel_type == 'A':\n",
    "        ylabel = \"Tilt (nrad)\"\n",
    "        yscale = 1e9\n",
    "    elif channel_type == 'H':\n",
    "        ylabel = \"Acceleration\\n(nm/s²)\"\n",
    "        yscale = 1e9\n",
    "    else:\n",
    "        ylabel = \"Amplitude\"\n",
    "        yscale = 1.0\n",
    "\n",
    "    components = ['Z', 'N', 'E']\n",
    "    font = 14\n",
    "\n",
    "    # Create 3 vertical subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize, sharex=True)\n",
    "    \n",
    "    # Colors for different models\n",
    "    colors = ['tab:red', 'tab:green', 'tab:blue', 'tab:orange', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray']\n",
    "    \n",
    "    # Plot each component\n",
    "    for i, comp in enumerate(components):\n",
    "        try:\n",
    "            # Get original data\n",
    "            orig_tr = seis_stream.select(channel=f\"*{comp}\").copy()[0]\n",
    "            \n",
    "            # Compute original spectrum\n",
    "            n = len(orig_tr.data)\n",
    "            win = windows.hann(n)\n",
    "            spec_orig = fft(orig_tr.data * win)\n",
    "            freq = fftfreq(n, d=orig_tr.stats.delta)\n",
    "            pos_freq = freq[0:n//2]\n",
    "            mag_orig = np.abs(spec_orig[0:n//2]) * 2.0/n * yscale\n",
    "            \n",
    "            # Apply octave band smoothing to original spectrum if requested\n",
    "            if smooth_octave:\n",
    "                pos_freq, mag_orig = smooth_octave_bands(pos_freq, mag_orig, \n",
    "                                                        octave_fraction, smooth_method)\n",
    "            \n",
    "            # Plot each model spectral difference\n",
    "            for j, (model_name, results) in enumerate(model_results.items()):\n",
    "                if comp in results['predicted_data']:\n",
    "                    residual_tr = seis_stream.select(channel=f\"*{comp}\").copy()[0]\n",
    "                    residual_tr.data = orig_tr.data - results['predicted_data'][comp]\n",
    "                    var_reduction = results['variance_reduction'][comp]\n",
    "                    \n",
    "                    # Compute residual spectrum\n",
    "                    spec_residual = fft(residual_tr.data * win)\n",
    "                    mag_residual = np.abs(spec_residual[0:n//2]) * 2.0/n * yscale\n",
    "                    freq_residual = freq[0:n//2]  # Use original frequency array\n",
    "                    \n",
    "                    # Apply octave band smoothing to residual spectrum if requested\n",
    "                    if smooth_octave:\n",
    "                        freq_residual, mag_residual = smooth_octave_bands(freq_residual, mag_residual, \n",
    "                                                                        octave_fraction, smooth_method)\n",
    "                    \n",
    "                    # For interpolation when smoothing is used\n",
    "                    if smooth_octave:\n",
    "                        from scipy.interpolate import interp1d\n",
    "                        # Interpolate residual spectrum to match original frequency grid\n",
    "                        f_interp = interp1d(freq_residual, mag_residual, kind='linear', \n",
    "                                          bounds_error=False, fill_value=0)\n",
    "                        mag_residual_interp = f_interp(pos_freq)\n",
    "                        \n",
    "                        # Compute spectral difference using interpolated residual\n",
    "                        mag_diff_db = 20 * np.log10(mag_residual_interp / mag_orig)\n",
    "                        \n",
    "                        # Apply frequency limits\n",
    "                        mask = (pos_freq >= fmin) & (pos_freq <= fmax)\n",
    "                        pos_freq_plot = pos_freq[mask]\n",
    "                        mag_diff_plot = mag_diff_db[mask]\n",
    "                    else:\n",
    "                        # No smoothing - use original arrays\n",
    "                        mag_diff_db = 20 * np.log10(mag_residual / mag_orig)\n",
    "                        \n",
    "                        # Apply frequency limits\n",
    "                        mask = (pos_freq >= fmin) & (pos_freq <= fmax)\n",
    "                        pos_freq_plot = pos_freq[mask]\n",
    "                        mag_diff_plot = mag_diff_db[mask]\n",
    "                    \n",
    "                    color = colors[j % len(colors)]\n",
    "                    axes[i].plot(pos_freq_plot, mag_diff_plot, color=color, linewidth=1.5, zorder=2,\n",
    "                                label=f'M{j+1} Spectral Diff (VR: {var_reduction:.1f}%)', alpha=0.9)\n",
    "            \n",
    "            # Format subplot\n",
    "            axes[i].set_ylabel(f\"Spectral Difference\\n(dB w.r.t. nrad/\" + r\"$\\sqrt{Hz}$\" + \")\", fontsize=font)\n",
    "            axes[i].legend(loc='lower left', fontsize=font-2, ncol=2)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].tick_params(labelsize=font-1)\n",
    "            \n",
    "            # Set scales\n",
    "            axes[i].set_xscale('log')\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting {comp}: {e}\")\n",
    "            axes[i].text(0.5, 0.5, f'Error loading {comp}', \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            continue\n",
    "    \n",
    "    # Set x-axis label\n",
    "    axes[-1].set_xlabel(\"Frequency (Hz)\", fontsize=font)\n",
    "    \n",
    "    # Set overall title\n",
    "    title = \"Spectral Difference Comparison (Original - Predicted) in dB\"\n",
    "    if smooth_octave:\n",
    "        title += f\" - Smoothed ({octave_fraction:.1f} octave, {smooth_method})\"\n",
    "    fig.suptitle(title, fontsize=font+2, fontweight='bold')\n",
    "    \n",
    "    # Add model names as text outside the frame\n",
    "    model_names = list(model_results.keys())\n",
    "    model_text = \"Models:  \" + \",   \".join([f\"{name}\" for i, name in enumerate(model_names)])\n",
    "    \n",
    "    # Add text below the plot\n",
    "    fig.text(0.5, 0.02, model_text, ha='center', va='bottom', fontsize=font-2, \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Adjust layout to make room for the model text\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_spectra_comparison(seis_stream, model_results, \n",
    "#                              smooth_octave=True, octave_fraction=1/2, smooth_method='median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ed38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
